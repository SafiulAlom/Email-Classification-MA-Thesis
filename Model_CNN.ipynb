{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skripte laden\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.chdir('/home/safiul/email-classification')\n",
    "import data\n",
    "from Email_Classification.Functions.packages import *\n",
    "import Email_Classification.Functions.Functions as fn\n",
    "import Email_Classification.Functions.Helper as hp\n",
    "import Email_Classification.Functions.parameters as par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameterwerte\n",
    "param_process = par.params['param_process']\n",
    "param_nlp = par.params['param_nlp']\n",
    "param_embed = par.params['param_embed']\n",
    "param_conv = par.params['param_conv']\n",
    "param_optimizer = par.params['param_optimizer']\n",
    "param_fit = par.params['param_fit']\n",
    "prior_params = par.params['prior_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Dateien laden\n",
    "headers = pd.read_parquet(data.HEADERS_FILE)\n",
    "bodies = pd.read_parquet(data.BODIES_FILE)\n",
    "targets = features.zedsets.prepare_target_most_common(\n",
    "    pd.read_parquet(data.TARGETS_FILE),\n",
    "    target_column='target_category',\n",
    "    num_most_common_class=100\n",
    ")\n",
    "emails = fn.createDataFrameEmail(headers = headers, \n",
    "                                 bodies = bodies, \n",
    "                                 targets = targets, \n",
    "                                 sample_size = param_process['sample_size'], \n",
    "                                 seed = 125)\n",
    "del headers, bodies, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Der Datensatz auf Test- und Trainingsmenge aufteilenX_train, X_test, y_train, y_test = \\\n",
    "model_selection.train_test_split(emails.emails, \n",
    "                                 emails.target_category, \n",
    "                                 test_size =  param_process['test_size'], \n",
    "                                 shuffle = True,\n",
    "                                 random_state = 123)\n",
    "#Konvertieren von kategorialen Labels in Zahlen\n",
    "LE = preprocessing.LabelEncoder()\n",
    "LE.fit(emails.target_category)\n",
    "y_train = LE.transform(y_train)\n",
    "y_test = LE.transform(y_test)\n",
    "encoder = LabelBinarizer()\n",
    "encoder_fit = encoder.fit(y_train)\n",
    "y_train = encoder_fit.transform(y_train)\n",
    "y_test = encoder_fit.transform(y_test)\n",
    "del emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Datenvorverarbeiten\n",
    "TP = fn.Text_process(nchar = param_process['nchar'], \n",
    "                     rmDigits = param_process['rmDigits'], \n",
    "                     trans_lower= param_process['trans_lower'])\n",
    "TP.fit(X_train)\n",
    "X_train_TP = fn.multiprocess_array(TP, X_train)\n",
    "X_test_TP = fn.multiprocess_array(TP, X_test)\n",
    "X_train_TP = pd.concat(X_train_TP)\n",
    "X_test_TP = pd.concat(X_test_TP)\n",
    "\n",
    "X_train_TP_ = X_train_TP[X_train_TP.apply(lambda x: len(x.split())) != 0]\n",
    "X_test_TP_ = X_test_TP[X_test_TP.apply(lambda x: len(x.split())) != 0]\n",
    "y_train = y_train[X_train_TP.apply(lambda x: len(x.split())) != 0]\n",
    "y_test = y_test[X_test_TP.apply(lambda x: len(x.split())) != 0]\n",
    "del X_train, X_test, X_train_TP, X_test_TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#tokenize + padding \n",
    "Embeddings = fn.Embedding_custom(max_feature = param_embed['input_dim'],\n",
    "                           X_train= X_train_TP_,\n",
    "                           X_test= X_test_TP_,\n",
    "                           quantile = param_embed['quantile'])\n",
    "MAXLEN, tokenizer, X_train_vec, X_test_vec =  Embeddings.padding()\n",
    "#Embedding-Matrix erstellen\n",
    "fb_model = load_facebook_model(\"fasttext/cc.de.300.bin\")\n",
    "embedding_matrix = Embeddings.embedding_matrix(fb_model)\n",
    "#with open('Email_Classification/Models/Sequential Models/rnn/lstm/Models/\\\n",
    "#2020-10-05_1932_acc_0.6889/feature/embedding_matrix.npy', 'rb') as f:\n",
    "#    embedding_matrix = np.load(f)\n",
    "del fb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#CNN-Modelle initialisieren\n",
    "CNN = fn.CNN_custom(param_embed = param_embed, \n",
    "               param_conv = param_conv,  \n",
    "               prior_params = prior_params,\n",
    "               param_optimizer = param_optimizer, \n",
    "               param_fit = param_fit,\n",
    "               run_vi = True,\n",
    "               embedding_matrix = embedding_matrix, \n",
    "               training = False, \n",
    "               MAXLEN = 396,\n",
    "               train_size = len(X_train_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Callbacks erstellen\n",
    "now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H%M\")\n",
    "wkd = param_fit['wkd']\n",
    "PATH_CHECKPOINTS = wkd + now\n",
    "if not os.path.isdir(PATH_CHECKPOINTS):\n",
    "    os.mkdir(PATH_CHECKPOINTS)\n",
    "    \n",
    "# Create callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor= param_fit['monitor'], \n",
    "                     patience=  param_fit['patience'], \n",
    "                     min_delta= param_fit['min_delta'], \n",
    "                     mode= param_fit['mode']\n",
    "),\n",
    "    ModelCheckpoint(\n",
    "        #see https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "        PATH_CHECKPOINTS +  \"/weights.best.hdf5\",\n",
    "        monitor=  param_fit['monitor'],\n",
    "        mode = param_fit['mode'],\n",
    "        save_best_only= param_fit['save_best_only'],\n",
    "        verbose= param_fit['verbose']\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Modell fitten\n",
    "steps_per_epoch = int(np.floor((len(X_train_vec) / param_fit['batch_size'])))\n",
    "print(f\"Model Params.\\nbatch_size: {param_fit['batch_size']}\\nEpochs: {param_fit['epochs']}\\n\"\n",
    "    f\"Step p. Epoch: {steps_per_epoch}\\n\"\n",
    ")\n",
    "\n",
    "story = CNN.fit(\n",
    "    X_train_vec,\n",
    "    y_train,\n",
    "    batch_size = param_fit['batch_size'],\n",
    "    epochs= param_fit['epochs'],\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(X_test_vec, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Das Mosell mit den besten Gewichte aktualisieren\n",
    "CNN.load_weights('Email_Classification/Models/Sequential Models/rnn/lstm/Checkpoint/2020-12-01_0124/weights.best.hdf5')\n",
    "score = CNN.evaluate(X_test_vec, \n",
    "                      y_test, \n",
    "                      verbose=1)\n",
    "print('Test loss:    ', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modell speichern\n",
    "path = 'Email_Classification/Models/Sequential Models/cnn/model/uncertainty/VI/phi_0.5_sig1_1.5_sig2_0.5'\n",
    "CNN.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainings- und Testmenge und Hyperparameterwerte speichern\n",
    "with open('Email_Classification/Models/Sequential Models/cnn/model/uncertainty/VI/phi_0.5_sig1_1.5_sig2_0.5/feature/X_train_vec.npy', 'wb') as f:\n",
    "    np.save(f, X_train_vec)\n",
    "with open('Email_Classification/Models/Sequential Models/cnn/model/uncertainty/VI/phi_0.5_sig1_1.5_sig2_0.5/feature/X_test_vec.npy', 'wb') as f:\n",
    "    np.save(f, X_test_vec)\n",
    "with open('Email_Classification/Models/Sequential Models/cnn/model/uncertainty/VI/phi_0.5_sig1_1.5_sig2_0.5/feature/y_train.npy', 'wb') as f:\n",
    "    np.save(f, y_train)\n",
    "with open('Email_Classification/Models/Sequential Models/cnn/model/uncertainty/VI/phi_0.5_sig1_1.5_sig2_0.5/feature/y_test.npy', 'wb') as f:\n",
    "    np.save(f, y_test)\n",
    "with open('Email_Classification/Models/Sequential Models/cnn/model/uncertainty/VI/phi_0.5_sig1_1.5_sig2_0.5/params.pkl', 'wb') as f:\n",
    "    pickle.dump(f, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
